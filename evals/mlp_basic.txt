Architecture: simple MLP with layers 64*3 -> 512 -> 256 -> 128 -> 64*13 (370K parameters)
Hyperparameters: Trained on dataset of 100000 boards for 10 epochs, batch size 128
Optimizer: Adam with learning rate 1e-3
Greater epoch #s (tested up to 50) don't yield better results

Default eval set (eval_2013-02.pt)
    Boards: 0.1809 (18085/100000)
    Squares: 0.9397 (6014395/6400000)
    Pieces: 0.8410 (1941071/2308047)
    Empty: 0.9954 (4073324/4091953)
    P: 0.8868 (554134/624846)
    R: 0.8731 (136328/156146)
    N: 0.7750 (80898/104380)
    B: 0.7425 (81042/109149)
    Q: 0.6849 (41808/61045)
    K: 0.8104 (79444/98031)
    p: 0.8800 (552456/627794)
    r: 0.8765 (134578/153540)
    n: 0.7500 (79223/105630)
    b: 0.7331 (82940/113139)
    q: 0.7014 (41703/59456)
    k: 0.8064 (76517/94891)
Correct colors: 0.7019 (70194/100000)
Regular pieces: 0.4796 (47958/100000)
Both: 0.3903 (39033/100000)

Alldata version: trained on the whole ~8 mil board dataset for 5 epochs, batch size 256
    Boards: 0.2522 (25222/100000)
    Squares: 0.9562 (6119656/6400000)
    Pieces: 0.8798 (2039787/2318598)
    Empty: 0.9996 (4079869/4081402)
    P: 0.8927 (580271/650030)
    R: 0.9006 (143708/159566)
    N: 0.8682 (84447/97269)
    B: 0.8456 (84862/100355)
    Q: 0.8145 (44630/54795)
    K: 0.8641 (84361/97629)
    p: 0.8946 (576085/643948)
    r: 0.8903 (142432/159983)
    n: 0.8497 (82370/96941)
    b: 0.8297 (90098/108590)
    q: 0.8457 (43796/51789)
    k: 0.8467 (82727/97703)
Correct colors: 0.9723 (97231/100000)
Regular pieces: 0.6447 (64471/100000)
Both: 0.6338 (63377/100000)
