1. Add autoregression
2. Deeper eval investigation: board legality, game accuracy over # moves played, square accuracy heatmap, attention maps?
3. Tidy up project: generate eval tables & plots, write up github


Other ideas:
Add embeddings/filter based on rating and move count. Can also use to make harder eval set.
Plot loss & accuracy curves on training & testing data
Automate "metadata" system for trained models in /evals


GPT's candidate SOTA approaches past standard transformer (prob won't bother):
- Add structural loss (test with MLP and transformer)
- Structured/graph transformer
- Autoregressive models